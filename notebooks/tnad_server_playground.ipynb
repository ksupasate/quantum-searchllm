{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNAD Server Playground\n",
    "\n",
    "Persona: Senior AI Researcher + Staff Software Engineer. This notebook wires the tensor-network augmented decoding (TNAD) stack into an interactive workflow that you can run on a high-memory server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "1. (Optional) Create and activate a virtual environment.\n",
    "   ```bash\n",
    "   python3 -m venv tnad-env\n",
    "   source tnad-env/bin/activate\n",
    "   ```\n",
    "2. Install Python dependencies (include `jupyter` if not preinstalled).\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "3. Launch the notebook server on the remote machine and tunnel the chosen port to your laptop.\n",
    "\n",
    "Once the runtime is ready, execute the cells below top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Bootstrap project context and optional runtime flags.\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"tnad\").exists():\n",
    "    raise RuntimeError(\"Run this notebook from the quantum-search-llm repository root.\")\n",
    "\n",
    "if not any(str(PROJECT_ROOT) == entry for entry in sys.path):\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# MPS on Apple Silicon can throw conservative OOMs; this mirrors the CLI guidance.\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# Core imports: Hugging Face model loader, TNAD primitives, evaluation helpers.\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tnad import FidelityGuidedBeamSearcher\n",
    "from tnad.mps_manager import MPSSequence\n",
    "from tnad.coherence_score import analyze_coherence_spectrum\n",
    "from tnad.utils import get_device\n",
    "\n",
    "from experiments.reproduce_paper_results import load_model_and_tokenizer\n",
    "from experiments.run_gsm8k import extract_answer_from_text as extract_gsm8k_answer\n",
    "from experiments.run_gsm8k import format_prompt as format_gsm8k_prompt\n",
    "from experiments.run_strategyqa import extract_yes_no_answer\n",
    "from experiments.run_strategyqa import format_prompt as format_strategyqa_prompt\n",
    "from experiments.run_strategyqa import load_strategyqa_dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configuration"
    ]
   },
   "outputs": [],
   "source": [
    "# Configure model + decoding defaults; tweak these to fit server capacity.\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # swap to a larger model if the server has sufficient memory\n",
    "USE_8BIT = False  # set True if bitsandbytes and GPU 8-bit inference are available\n",
    "FGBS_CONFIG: Dict[str, Any] = {\n",
    "    \"beam_width\": 3,\n",
    "    \"alpha\": 0.5,\n",
    "    \"bond_dim\": 16,\n",
    "    \"top_k\": 40,\n",
    "    \"temperature\": 1.0,\n",
    "    \"normalize_embeddings\": True,\n",
    "}\n",
    "GENERATION_CONFIG: Dict[str, Any] = {\n",
    "    \"max_length\": 256,\n",
    "    \"min_length\": 8,\n",
    "    \"return_details\": True,\n",
    "    \"show_progress\": False,\n",
    "}\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"auto\",\n",
    "    torch_dtype_name=\"float16\",\n",
    "    load_in_8bit=USE_8BIT,\n",
    ")\n",
    "\n",
    "searcher = FidelityGuidedBeamSearcher(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    beam_width=FGBS_CONFIG[\"beam_width\"],\n",
    "    alpha=FGBS_CONFIG[\"alpha\"],\n",
    "    bond_dim=FGBS_CONFIG[\"bond_dim\"],\n",
    "    top_k=FGBS_CONFIG[\"top_k\"],\n",
    "    temperature=FGBS_CONFIG[\"temperature\"],\n",
    "    normalize_embeddings=FGBS_CONFIG[\"normalize_embeddings\"],\n",
    ")\n",
    "\n",
    "device_info = get_device()\n",
    "print(f\"Active device: {device_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "demo"
    ]
   },
   "outputs": [],
   "source": [
    "# Quick sanity check: run FGBS on a single reasoning prompt.\n",
    "demo_prompt = \"Q: If A > B and B > C, what can we conclude about A and C?\\nA:\"\n",
    "demo_result = searcher.generate(demo_prompt, **GENERATION_CONFIG)\n",
    "print(\"Generated text:\\n\", demo_result[\"text\"])\n",
    "print(\"\\nScores:\")\n",
    "print(f\"  log_prob      : {demo_result['log_prob']:.4f}\")\n",
    "print(f\"  log_cfs       : {demo_result['log_cfs']:.4f}\")\n",
    "print(f\"  composite_score: {demo_result['composite_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "evaluation"
    ]
   },
   "outputs": [],
   "source": [
    "# Helper routines for batched evaluation on GSM8K (math) and StrategyQA (yes/no).\n",
    "def evaluate_gsm8k_subset(\n",
    "    searcher: FidelityGuidedBeamSearcher,\n",
    "    num_examples: int = 5,\n",
    "    split: str = \"test\",\n",
    "    prompt_template: str = \"Q: {question}\\nA: Let's think step by step.\",\n",
    "    max_length: int = 256,\n",
    ") -> Dict[str, Any]:\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\", split=split)\n",
    "    if num_examples > 0:\n",
    "        dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    correct = 0\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        prompt = format_gsm8k_prompt(sample[\"question\"], prompt_template)\n",
    "        result = searcher.generate(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            min_length=GENERATION_CONFIG[\"min_length\"],\n",
    "            return_details=False,\n",
    "            show_progress=False,\n",
    "        )\n",
    "        predicted = extract_gsm8k_answer(result[\"text\"])\n",
    "        gold = extract_gsm8k_answer(sample[\"answer\"])\n",
    "        is_correct = (predicted == gold)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        rows.append(\n",
    "            {\n",
    "                \"example_id\": idx,\n",
    "                \"question\": sample[\"question\"],\n",
    "                \"gold_answer\": gold,\n",
    "                \"predicted_answer\": predicted,\n",
    "                \"correct\": is_correct,\n",
    "                \"generated_text\": result[\"text\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracy = correct / len(rows) if rows else 0.0\n",
    "    return {\n",
    "        \"dataset\": f\"gsm8k::{split}\",\n",
    "        \"num_examples\": len(rows),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"records\": rows,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_strategyqa_subset(\n",
    "    searcher: FidelityGuidedBeamSearcher,\n",
    "    num_examples: int = 5,\n",
    "    split: str = \"validation\",\n",
    "    prompt_template: str = \"Question: {question}\\nAnswer (yes/no):\",\n",
    "    max_length: int = 128,\n",
    ") -> Dict[str, Any]:\n",
    "    loader_config = {\n",
    "        \"dataset\": {\n",
    "            \"split\": split,\n",
    "            \"strategyqa\": {\n",
    "                \"prompt_template\": prompt_template,\n",
    "                \"hub_ids\": [\n",
    "                    \"wics/strategy-qa\",\n",
    "                    \"wics/strategyqa\",\n",
    "                    \"strategy_qa\",\n",
    "                ],\n",
    "                \"local_path\": \"data/strategyqa_sample.jsonl\",\n",
    "            },\n",
    "        },\n",
    "        \"generation\": {\n",
    "            \"max_length\": max_length,\n",
    "            \"min_length\": 6,\n",
    "            \"return_details\": False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    dataset = load_strategyqa_dataset(loader_config)\n",
    "    if num_examples > 0:\n",
    "        dataset = dataset.select(range(min(num_examples, len(dataset))))\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    correct = 0\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        prompt = format_strategyqa_prompt(sample[\"question\"], prompt_template)\n",
    "        result = searcher.generate(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            min_length=loader_config[\"generation\"][\"min_length\"],\n",
    "            return_details=False,\n",
    "            show_progress=False,\n",
    "        )\n",
    "        predicted_text = extract_yes_no_answer(result[\"text\"])\n",
    "        gold_raw = sample[\"answer\"]\n",
    "        if isinstance(gold_raw, str):\n",
    "            gold_bool = gold_raw.strip().lower() in {\"yes\", \"true\", \"1\"}\n",
    "        else:\n",
    "            gold_bool = bool(gold_raw)\n",
    "        predicted_bool: Optional[bool]\n",
    "        if predicted_text is None:\n",
    "            predicted_bool = None\n",
    "        else:\n",
    "            predicted_bool = predicted_text == \"yes\"\n",
    "        is_correct = (predicted_bool == gold_bool)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        rows.append(\n",
    "            {\n",
    "                \"example_id\": idx,\n",
    "                \"question\": sample[\"question\"],\n",
    "                \"gold_answer\": gold_bool,\n",
    "                \"predicted_answer\": predicted_bool,\n",
    "                \"generated_text\": result[\"text\"],\n",
    "                \"correct\": is_correct,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracy = correct / len(rows) if rows else 0.0\n",
    "    return {\n",
    "        \"dataset\": f\"strategyqa::{split}\",\n",
    "        \"num_examples\": len(rows),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"records\": rows,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Helper functions ready; call them in the next cell to benchmark subsets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "benchmark"
    ]
   },
   "outputs": [],
   "source": [
    "# Run subset evaluations (adjust counts upward once the server proves stable).\n",
    "gsm8k_metrics = evaluate_gsm8k_subset(searcher, num_examples=3)\n",
    "strategyqa_metrics = evaluate_strategyqa_subset(searcher, num_examples=3)\n",
    "\n",
    "print(\"GSM8K subset accuracy:\", f\"{gsm8k_metrics['accuracy']*100:.1f}%\")\n",
    "print(\"StrategyQA subset accuracy:\", f\"{strategyqa_metrics['accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "analytics"
    ]
   },
   "outputs": [],
   "source": [
    "# Inspect coherence dynamics for the last decoded trace.\n",
    "if gsm8k_metrics[\"records\"]:\n",
    "    full_text = gsm8k_metrics[\"records\"][-1][\"generated_text\"]\n",
    "    mps = MPSSequence(bond_dim=FGBS_CONFIG[\"bond_dim\"], embedding_dim=searcher.embedding_dim)\n",
    "    with torch.no_grad():\n",
    "        token_ids = tokenizer.encode(full_text, return_tensors=\"pt\")[0].to(searcher.device)\n",
    "        embeddings = searcher.embedding_layer(token_ids)\n",
    "    for embedding in embeddings:\n",
    "        mps.add_token(embedding)\n",
    "    spectrum = mps.get_schmidt_values()\n",
    "    coherence_report = analyze_coherence_spectrum(spectrum)\n",
    "    print(\"Schmidt spectrum (first five values):\", spectrum[:5])\n",
    "    print(\"Coherence fidelity:\", coherence_report[\"cfs\"])\n",
    "else:\n",
    "    print(\"No GSM8K records available for coherence inspection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "cleanup"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional cleanup to release GPU memory before another experiment.\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"Memory cache cleared (if CUDA was active).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
