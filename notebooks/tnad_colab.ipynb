{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNAD Colab Runner (GPU)\n",
    "\n",
    "End-to-end notebook for executing the Tensor Network-Augmented Decoding (TNAD) experiments on Google Colab with CUDA-enabled GPUs. Follow the cells sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Runtime Preparation\n",
    "\n",
    "1. In Colab, switch the runtime to **GPU**: `Runtime → Change runtime type → Hardware accelerator → GPU`.\n",
    "2. Execute the cells below in order. Installation steps may take a few minutes the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Verify that a CUDA GPU is visible to PyTorch.\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA available  : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU name        : {gpu.name}\")\n",
    "    print(f\"Total memory    : {gpu.total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    raise EnvironmentError(\"A CUDA-enabled runtime is required. Switch the Colab runtime to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pip\n",
    "!pip install -q accelerate bitsandbytes datasets loguru matplotlib pyyaml seaborn sentencepiece tqdm transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the Project\n",
    "\n",
    "Clones the repository that contains the TNAD source code. Update `REPO_URL` if your fork lives at a different location. If you have already uploaded the files manually, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "clone"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/your-org/quantum-search-llm.git\"  # TODO: replace with your repository URL\n",
    "REPO_DIR = Path(\"quantum-search-llm\")\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    print(f\"Repository already present at {REPO_DIR.resolve()}\")\n",
    "else:\n",
    "    !git clone --depth 1 {REPO_URL}\n",
    "    print(\"Clone complete.\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the project in editable mode so notebooks can import the TNAD package.\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Experiment Parameters\n",
    "\n",
    "Adjust the variables below to control which model runs, how many benchmark samples are evaluated, and the FGBS hyperparameters. The defaults are chosen to fit comfortably within a Colab T4/A100 session while still exercising the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "config"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-2\"          # Use a 2.7B model that fits easily on Colab GPUs.\n",
    "USE_8BIT = True                         # Requires bitsandbytes (installed above). Keeps VRAM usage low for larger models.\n",
    "ALPHA = 0.5                             # Fluency vs coherence balance.\n",
    "BOND_DIM = 16                           # Logical bandwidth χ.\n",
    "BEAM_WIDTH = 5                          # Number of beams.\n",
    "NUM_EXAMPLES = 25                       # Evaluate this many samples per benchmark (set to -1 for full dataset).\n",
    "\n",
    "BASE_CONFIG_PATH = Path(\"configs/default.yaml\")\n",
    "COLAB_CONFIG_PATH = Path(\"configs/colab_autogen.yaml\")\n",
    "\n",
    "with BASE_CONFIG_PATH.open(\"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config[\"model\"][\"name\"] = MODEL_NAME\n",
    "config[\"model\"][\"device\"] = \"cuda\"\n",
    "config[\"model\"][\"load_in_8bit\"] = bool(USE_8BIT)\n",
    "config[\"model\"][\"torch_dtype\"] = \"float16\"\n",
    "\n",
    "config[\"fgbs\"][\"alpha\"] = float(ALPHA)\n",
    "config[\"fgbs\"][\"bond_dim\"] = int(BOND_DIM)\n",
    "config[\"fgbs\"][\"beam_width\"] = int(BEAM_WIDTH)\n",
    "\n",
    "config[\"experiment\"][\"num_examples\"] = int(NUM_EXAMPLES)\n",
    "\n",
    "if COLAB_CONFIG_PATH.exists():\n",
    "    COLAB_CONFIG_PATH.unlink()\n",
    "\n",
    "with COLAB_CONFIG_PATH.open(\"w\") as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "\n",
    "print(f\"Wrote Colab-specific configuration to {COLAB_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Reproduction Script on GPU\n",
    "\n",
    "This cell executes the main experiment driver using the Colab-friendly configuration. Logs and tables will be stored under `paper_results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "run"
    ]
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"experiments/reproduce_paper_results.py\",\n",
    "    \"--model\", MODEL_NAME,\n",
    "    \"--num_examples\", str(NUM_EXAMPLES),\n",
    "    \"--alpha\", str(ALPHA),\n",
    "    \"--bond_dim\", str(BOND_DIM),\n",
    "    \"--config\", str(COLAB_CONFIG_PATH),\n",
    "]\n",
    "\n",
    "print(\"Running command:\\n\", \" \".join(cmd))\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Results\n",
    "\n",
    "The reproduction script saves both raw JSON and formatted tables. The cell below displays the latest run summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "results"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "results_dir = Path(\"paper_results\")\n",
    "table_files = sorted(results_dir.glob(\"tables_*.txt\"))\n",
    "if not table_files:\n",
    "    raise FileNotFoundError(\"No results found. Ensure the previous cell completed successfully.\")\n",
    "\n",
    "latest_tables = table_files[-1]\n",
    "print(f\"Displaying summary from {latest_tables}\")\n",
    "print(latest_tables.read_text())\n",
    "\n",
    "json_files = sorted(results_dir.glob(\"full_results_*.json\"))\n",
    "if json_files:\n",
    "    latest_json = json_files[-1]\n",
    "    with latest_json.open(\"r\") as f:\n",
    "        payload = json.load(f)\n",
    "    print(\"\\nStored result keys:\", list(payload.keys()))\n",
    "else:\n",
    "    print(\"No JSON artifact found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional: Interactive Generation\n",
    "\n",
    "Use TNAD directly for ad-hoc reasoning experiments once the model is loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "interactive"
    ]
   },
   "outputs": [],
   "source": [
    "from tnad import FidelityGuidedBeamSearcher\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    load_in_8bit=bool(USE_8BIT),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "interactive_searcher = FidelityGuidedBeamSearcher(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    beam_width=BEAM_WIDTH,\n",
    "    alpha=ALPHA,\n",
    "    bond_dim=BOND_DIM,\n",
    ")\n",
    "\n",
    "prompt = \"Solve: If a train travels 60 miles in 1.5 hours, what is its average speed in miles per hour?\\nAnswer:\"\n",
    "result = interactive_searcher.generate(prompt, max_length=128, min_length=12, return_details=False)\n",
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
